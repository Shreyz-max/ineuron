{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular types of Loss Funtions\n",
    "\n",
    "A loss function is a method of evaluating how well your algorithm models your dataset. If your predictions are totally off, your loss function will output a higher number. If they're pretty good, it'll output a lower number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Least Square Error : \n",
    "Least Squared error also know as L2 Loss is the sum of the squared of the differences between the actual and predicted values. This error is usually used for regression problems.\n",
    "\n",
    "![title](img/l1.png)\n",
    "\n",
    "\n",
    "This error penalises heavily to outliers since it is the squre of errors. Thus if our data contains a lot of outliers then this error should not be used. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Least Absolute Error :\n",
    "\n",
    "Least Absolute error also known as L1 error is the absolute difference between the actual and predicted values. This error is also used for regression problems. It is more robust to outliers since unlike LSE it does not penalise outliers heavily. \n",
    "\n",
    "![title](img/l2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Huber Loss:\n",
    "\n",
    "Huber loss combines the best of L1 and L2 losses. It is quadratic for small errors and for larger errors it is linear.\n",
    "\n",
    "$ \\begin{equation}\n",
    "  L_{\\delta}=\\begin{cases}\n",
    "    \\frac{1}{2} (y - f(x))^{2}, & \\text{if | y - f(x) |<=}\\delta .\\\\\n",
    "    \\delta |y - f(x)| - \\frac{1}{2}\\delta^{2}, & \\text{if | y - f(x) |>}\\delta.\\\\\n",
    "  \\end{cases} \n",
    "  \\end{equation}$\n",
    "  \n",
    "It is also more robust to outliers than LSE.\n",
    "\n",
    "### The graph for huber loss for various values is:\n",
    "\n",
    "![title](img/huber.jpeg)\n",
    "\n",
    "It allows for large gradients for large numbers and smaller gradient for small numbers. Also computing $\\delta$ is computationally expensive so we use  Log-Cosh Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hinge Loss :\n",
    "\n",
    "This loss is usually used in SVM classification with class labels -1 and 1.  \n",
    "\n",
    "![title](img/hinge.jpg)\n",
    "\n",
    "It penalises not only wrong prediction but also no so confident right predictions.\n",
    "\n",
    "### The resulting graph is:\n",
    "\n",
    "![title](img/hinge.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Binary Cross Entropy :\n",
    "\n",
    "We use binary cross entropy to minimise loss for models which output a probablility $\\hat{y_{i}}$. \n",
    "\n",
    "![title](img/BinaryCrossEntropy.png)\n",
    "\n",
    "It is also known as log loss. This is used for binary classification as the name suggests.\n",
    "\n",
    "### The resulting graph is:\n",
    "\n",
    "![title](img/logloss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MultiClass Cross Entropy :\n",
    "\n",
    "![title](img/mce.jpg)\n",
    "\n",
    "This is a generalisation of binary cross entropy over more than two classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. KL-Divergence\n",
    "\n",
    "The Kullback-Liebler Divergence is a measure of how a probability distribution differs from another distribution. A KL-divergence of zero indicates that the distributions are identical.\n",
    "\n",
    "![title](img/kl.jpg)\n",
    "\n",
    "KL-Divergence cannot be used as a distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exponential Loss\n",
    "\n",
    "The exponential loss was designed at the beginning of the Adaboost algorithm which greedily optimized it. The mathematical form is:\n",
    "\n",
    "![title](img/exp.png) \n",
    "\n",
    "### The resulting graph is:\n",
    "\n",
    "![title](img/exp.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Log-Cosh Loss :\n",
    "\n",
    "This is another type of loss function used in regression. It is smoother than L2 loss. Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "![title](img/logcosh.png)\n",
    "\n",
    "log(cosh(x)) is approximately equal to $x^{2}/ 2 $for small x and to abs(x) - log(2) for large x. This means that 'logcosh' works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction. \n",
    "\n",
    "### The resulting graph of the loss is:\n",
    "\n",
    "![title](img/logcosh1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Quantile regression loss function :\n",
    "\n",
    "This is a regression loss function that is applied to predict quantiles. A quantile determines how many values in a distribution are above or below a certain limit.\n",
    "\n",
    "Given a prediction $y_{i}^{p}$ and outcome $y_{i}$, the mean regression loss for a quantile q is:\n",
    "\n",
    "![title](img/quantile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular types of activation functions\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network. The function is attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron's input is relevant for the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sigmoid Function:\n",
    "\n",
    "A sigmoid function is a bounded, differentiable, real function that is defined for all real input values and has a non-negative derivative at each point and exactly one inflection point. A sigmoid “function” and a sigmoid “curve” refer to the same object. Additionally, the sigmoid function is not symmetric around zero.\n",
    "\n",
    "![title](img/sigmoid_function.svg)\n",
    "\n",
    "### The resulting graph is :\n",
    "\n",
    "![title](img/Sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tanh\n",
    "\n",
    "Tanh or hyperbolic tangent Activation Function is also like a logistic sigmoid but better. The range of tanh function is from -1 to 1, tanh is also sigmoidal (s-shaped).Tanh function is mainly used for classification between two classes. It is monotonic while its derivative is not monotonic. It is also differentiable. \n",
    "\n",
    "![title](img/tanh.png)\n",
    "\n",
    "### The resulting graph is:\n",
    "\n",
    "![title](img/TanH.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ReLU: \n",
    "\n",
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better. The function and its derivative both are monotonic. The ReLU is the most used activation function in the world right now. Since, it is used in almost all the convolutional neural networks or deep learning.\n",
    "\n",
    "![title](img/relu.png)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leaky ReLU:\n",
    "\n",
    "Leaky ReLUs are one attempt to fix the “dying ReLu” problem. Instead of the function being zero x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so). Both leaky ReLU function and its derivative are monotonic in nature. The function computes \n",
    "\n",
    "![title](img/lrelu.png)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/Leaky_ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameterised ReLu:\n",
    "\n",
    "Parameterized ReLU or Parametric ReLU activation function is a variant ReLU. It is similar to Leaky ReLU, with a slight change in dealing with negative input values. It aims to solve the problem of gradient’s becoming zero for the left half of the axis. It introduces a new parameter as a slope of the negative part of the function.\n",
    "\n",
    "![title](img/prelu.png)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/Parametric_ReLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exponential Linear Unit:\n",
    "\n",
    "Exponential Linear Unit or ELU for short is also a variant of Rectified Linear Unit (ReLU) that modifies the slope of the negative part of the function. Unlike the leaky ReLU and parametric ReLU functions, instead of a straight line, ELU uses  a long curve for defining the negative values.\n",
    "\n",
    "![title](img/elu.svg)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/Exponential_LU.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Swish:\n",
    "\n",
    "Swish is a lesser known activation function which was discovered by researchers at Google. Swish is as computationally efficient as ReLU and shows better performance than ReLU on deeper models. The values for swish ranges from negative infinity to infinity. This function is not monotonic.\n",
    "\n",
    "![title](img/swish.svg)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/Swish.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Softmax:\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "![title](img/softmaxequation.jpg)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/softmax.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Softplus:\n",
    "\n",
    "The softmax function, also known as softargmax or normalized exponential function, is a generalization of the logistic function to multiple dimensions. It is used in multinomial logistic regression and is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes.\n",
    "\n",
    "![title](img/softplus.svg)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/activation-softplus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SoftSign:\n",
    "\n",
    "The Softsign function is an activation function which rescales the values between -1 and 1 by applying a threshold just like a sigmoid function. The advantage, that is, the value of a softsign is zero-centered which helps the next neuron during propagating.\n",
    "\n",
    "![title](img/softsign.svg)\n",
    "\n",
    "### The resulting graph is: \n",
    "\n",
    "![title](img/softsign.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
